{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCBERT only"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Steps:\n",
    "1. Use LEBERT train from SUPERNER datasets.\n",
    "2. Predict target domain train dataset.\n",
    "3. Use predict dataset train P-BERT.\n",
    "4. Load P-BERT and fine tune C-BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict target domain train dataset.\n",
    "from CC.predicter import NERPredict\n",
    "import json\n",
    "\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/config.json', # chinese_wwm_ext\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "\n",
    "    'hidden_dim': 300, \n",
    "\n",
    "    'max_seq_length': 150, # max_length 150 , cut all datasets\n",
    "    'max_scan_num': 1000000, # embedding size LEBERT\n",
    "\n",
    "    'train_file': 'data/SuperNER_note4/train.json', # predict model TRAIN dataset\n",
    "    'eval_file': 'data/weibo/dev.json', # predict model eval dataset\n",
    "    'test_file': 'data/weibo/test.json', # test model test dataset\n",
    "    \n",
    "    'tag_file': 'data/SuperNER_note4/labels.txt',\n",
    "\n",
    "    'output_eval': True,\n",
    "\n",
    "    'loader_name': 'le_loader',\n",
    "\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "\n",
    "    \"default_tag\": \"O\",\n",
    "\n",
    "    'batch_size': 64,\n",
    "    'eval_batch_size': 512,\n",
    "\n",
    "    'do_shuffle': True,\n",
    "    'model_name': 'LEBert',\n",
    "\n",
    "    'task_name': 'origin_super_note4_predict_model' # replace your task_name\n",
    "}\n",
    "\n",
    "# load trained model\n",
    "args[\"lstm_crf_model_file\"] = \"save_model/super_predict_model/lstm_crf/lstm_crf_66930.pth\"\n",
    "args[\"bert_model_file\"] = \"save_model/super_predict_model/LEBert/LEBert_66930.pth\"\n",
    "\n",
    "predict = NERPredict(**args)\n",
    "\n",
    "\n",
    "def predict_train(train_file, saved_file):\n",
    "    batch_size = 64\n",
    "    index = 0\n",
    "    sentences = []\n",
    "\n",
    "    with open(saved_file, \"w\", encoding=\"utf-8\") as out:\n",
    "        with open(train_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                text = data[\"text\"]\n",
    "\n",
    "                sentences.append(text)\n",
    "                index += 1\n",
    "                if index % batch_size == batch_size-1:\n",
    "                    for s, label in predict(sentences):\n",
    "                        assert len(s[:args[\"max_seq_length\"]-2]) == len(label),f\"{s} {label} {len(s)} {len(label)}\"\n",
    "                        out.write(\n",
    "                            f\"\"\"{json.dumps({\"text\":s[:args[\"max_seq_length\"]-2],\"label\":label},ensure_ascii=False)}\\n\"\"\")\n",
    "                    sentences = []\n",
    "                    out.flush()\n",
    "            # last sentence\n",
    "            if len(sentences) > 0:\n",
    "                for s, label in predict(sentences):\n",
    "                    assert len(s[:args[\"max_seq_length\"]]) == len(label),f\"{s} {label} {len(s)} {len(label)}\"\n",
    "                    out.write(\n",
    "                        f\"\"\"{json.dumps({\"text\":s[:args[\"max_seq_length\"]-2],\"label\":label},ensure_ascii=False)}\\n\"\"\")\n",
    "\n",
    "# datasets\n",
    "trainsets = [\"weibo\",\"note4\",\"resume\",\"msra\"]\n",
    "# scales\n",
    "scales = [250,500,1000,1350]\n",
    "# datasets root path\n",
    "path = f\"data/few_shot\"\n",
    "\n",
    "for dataname in trainsets:\n",
    "    for scale in scales:\n",
    "        # origin train path\n",
    "        train_file = f\"{path}/{dataname}/train_{scale}.json\"\n",
    "        # output path\n",
    "        saved_file = f\"{path}/{dataname}/train_{scale}_super_note4_pred.json\"\n",
    "        predict_train(train_file,saved_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.merge_json import merge_labels\n",
    "\n",
    "labels_origin = [\n",
    "    \"data/few_shot/weibo/labels.txt\",\n",
    "    \"data/few_shot/note4/labels.txt\",\n",
    "    \"data/few_shot/resume/labels.txt\",\n",
    "    \"data/few_shot/msra/labels.txt\"\n",
    "]\n",
    "# merge labels\n",
    "labels = [\"data/SuperNER/tags_list.txt\", \"data/lebert/dataset/NER/note4/labels.txt\"]\n",
    "\n",
    "for origin in labels_origin:\n",
    "    # remain label ordered\n",
    "    merge_labels([origin]+labels,origin.replace(\"labels.txt\",\"super_labels.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use predict dataset train P-BERT.\n",
    "# LTS branch\n",
    "pretrain_model_args = {\n",
    "    'num_epochs': 35,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "\n",
    "    'train_file': './data/few_shot/weibo/train_250_super_note4_pred.json',\n",
    "    'eval_file': './data/few_shot/weibo/dev.json',\n",
    "    'test_file': './data/few_shot/weibo/test.json',\n",
    "    \n",
    "    'tag_file': 'data/few_shot/weibo/super_labels.txt',\n",
    "\n",
    "    'loader_name': 'ptloader_v2',\n",
    "\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 32,\n",
    "    'pass_none_rule': True,\n",
    "    'skip_single_matched_word': True,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'weibo_pbert_pretrain_250', # replace your task name\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "\n",
    "    \"tag_rules\": {\n",
    "        \"O\": \"非实体\",\n",
    "        \"PER.NOM\": \"指代人名\",\n",
    "        \"LOC.NAM\": \"地名\",\n",
    "        \"PER.NAM\": \"人名\",\n",
    "        \"GPE.NAM\": \"政体\",\n",
    "        \"ORG.NAM\": \"机构\",\n",
    "        \"ORG.NOM\": \"指代机构\",\n",
    "        \"LOC.NOM\": \"指代地名\",\n",
    "        \"GPE.NOM\": \"指代政体\",\n",
    "        \"NR\": \"人名\",\n",
    "        \"NS\": \"地名\",\n",
    "        \"NT\": \"组织机构\",\n",
    "        \"CONT\": \"国家\",\n",
    "        \"PRO\": \"职位\",\n",
    "        \"RACE\": \"种族\",\n",
    "        \"TITLE\": \"工作名称\",\n",
    "        \"EDU\": \"教育经历\",\n",
    "        \"NAME\": \"名字\",\n",
    "        \"ORG\": \"机构\",\n",
    "        \"LOC\": \"地名\",\n",
    "        \"PER\": \"人名\",\n",
    "        \"GPE\": \"政治实体\",\n",
    "        \"Time\": \"时间\",\n",
    "        \"Thing\": \"物品\",\n",
    "        \"Metric\": \"度量\",\n",
    "        \"Abstract\": \"作品\",\n",
    "        \"Physical\": \"实体\",\n",
    "        \"Term\": \"术语\",\n",
    "        \"company\": \"企业\",\n",
    "        \"name\": \"名字\",\n",
    "        \"game\": \"游戏\",\n",
    "        \"movie\": \"电影\",\n",
    "        \"position\": \"职位\",\n",
    "        \"address\": \"地址\",\n",
    "        \"government\": \"政府\",\n",
    "        \"scene\": \"景点\",\n",
    "        \"book\": \"书名\",\n",
    "        \"NORP\": \"政体民族\",\n",
    "        \"PERSON\": \"人名\",\n",
    "    }\n",
    "}\n",
    "\n",
    "import torch\n",
    "from CC.pre_trained import NERPreTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"weibo\",\"note4\",\"resume\",\"msra\"]\n",
    "\n",
    "scales = [250,500,1000,1350]\n",
    "\n",
    "tasks_args = {\n",
    "        \"train_file\":\"./data/few_shot/{0}/train_{1}_super_note4_pred.json\",\n",
    "        \"eval_file\":'./data/few_shot/{0}/dev.json',\n",
    "        \"test_file\":'./data/few_shot/{0}/test.json',\n",
    "        \"tag_file\":\"data/few_shot/{0}/super_labels.txt\",\n",
    "        'task_name': '{0}_pbert_pretrain_{1}'\n",
    "    }\n",
    "\n",
    "for name in datasets:\n",
    "    for scale in scales:\n",
    "        for k,v in tasks_args.items():\n",
    "            pretrain_model_args[k] = v.format(name,scale)\n",
    "        pre_trainer = NERPreTrainer(**pretrain_model_args)\n",
    "\n",
    "        for i in pre_trainer():\n",
    "            a = i\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2,3],\n",
    "\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/config.json',\n",
    "    'pretrained_file_name': '.save_pretrained/weibo_multiple_pretrained/Bert_5915/pytorch_model.bin',\n",
    "    'prompt_pretrained_file_name': 'save_pretrained/weibo_multiple_pretrained/Bert_5915/pytorch_model.bin',\n",
    "    'prompt_config_file_name': 'save_pretrained/weibo_multiple_pretrained/Bert_5915/config.json',\n",
    "\n",
    "    'hidden_dim': 300, # CRF LSTM\n",
    "#    \"hidden_dim\":768,\n",
    "\n",
    "    'max_seq_length': 150,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/weibo/train.json',\n",
    "    'eval_file': './data/weibo/dev.json',\n",
    "    'test_file': './data/weibo/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/weibo/labels.txt',\n",
    "    'output_eval': True,\n",
    "    'loader_name': 'ft_loader_v4',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 32,\n",
    "    'do_shuffle': True,\n",
    "    'model_name': 'LEBert',\n",
    "\n",
    "    \"tag_rules\": {\n",
    "        \"O\": \"非实体\",\n",
    "        \"PER.NOM\": \"指代人名\",\n",
    "        \"LOC.NAM\": \"地名\",\n",
    "        \"PER.NAM\": \"人名\",\n",
    "        \"GPE.NAM\": \"政体\",\n",
    "        \"ORG.NAM\": \"机构\",\n",
    "        \"ORG.NOM\": \"指代机构\",\n",
    "        \"LOC.NOM\": \"指代地名\",\n",
    "        \"GPE.NOM\": \"指代政体\",\n",
    "        \"NR\": \"人名\",\n",
    "        \"NS\": \"地名\",\n",
    "        \"NT\": \"组织机构\",\n",
    "        \"CONT\": \"国家\",\n",
    "        \"PRO\": \"职位\",\n",
    "        \"RACE\": \"种族\",\n",
    "        \"TITLE\": \"工作名称\",\n",
    "        \"EDU\": \"教育经历\",\n",
    "        \"NAME\": \"名字\",\n",
    "        \"ORG\": \"机构\",\n",
    "        \"LOC\": \"地名\",\n",
    "        \"PER\": \"人名\",\n",
    "        \"GPE\": \"政治实体\",\n",
    "        \"Time\": \"时间\",\n",
    "        \"Thing\": \"物品\",\n",
    "        \"Metric\": \"度量\",\n",
    "        \"Abstract\": \"作品\",\n",
    "        \"Physical\": \"实体\",\n",
    "        \"Term\": \"术语\",\n",
    "        \"company\": \"企业\",\n",
    "        \"name\": \"名字\",\n",
    "        \"game\": \"游戏\",\n",
    "        \"movie\": \"电影\",\n",
    "        \"position\": \"职位\",\n",
    "        \"address\": \"地址\",\n",
    "        \"government\": \"政府\",\n",
    "        \"scene\": \"景点\",\n",
    "        \"book\": \"书名\",\n",
    "        \"NORP\": \"政体民族\",\n",
    "        \"PERSON\": \"人名\",\n",
    "    },\n",
    "    'task_name': 'weibo_tag_multiple_3'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datasets = [\"weibo\"]\n",
    "scales = [1350]\n",
    "steps = [5985]\n",
    "# scales = [250]\n",
    "# steps = [1120]\n",
    "# scales = [250,500,1000,1350]\n",
    "# steps = [1120,2205,4375,5915]\n",
    "\n",
    "# resume\n",
    "# steps = [1120,2240,4410,5950]\n",
    "\n",
    "# weibo\n",
    "# steps = [1120,2240,4445,5985]\n",
    "\n",
    "per_task_count = 1\n",
    "\n",
    "tasks_args = {\n",
    "    \"task_name\": \"{0}_pc_bert_crf_{1}_{2}\",\n",
    "    \n",
    "    'prompt_config_file_name':\"save_pretrained/{0}_pbert_pretrain_{1}/Bert_{3}/config.json\",\n",
    "    \"pretrained_file_name\":\"save_pretrained/{0}_pbert_pretrain_{1}/Bert_{3}/pytorch_model.bin\",\n",
    "    'prompt_pretrained_file_name':\"save_pretrained/{0}_pbert_pretrain_{1}/Bert_{3}/pytorch_model.bin\",\n",
    "\n",
    "    'train_file': './data/few_shot/{0}/train_{1}.json',\n",
    "    'eval_file': './data/few_shot/{0}/dev.json',\n",
    "    'test_file': './data/few_shot/{0}/test.json',\n",
    "    'tag_file':\"./data/few_shot/{0}/labels.txt\"\n",
    "}\n",
    "from CC.enhanced_trainer import EnhancedNERTrainer\n",
    "import torch\n",
    "\n",
    "for dataset in datasets:\n",
    "    for scale,step in zip(scales,steps):\n",
    "        for i in range(per_task_count):\n",
    "            for k,v in tasks_args.items():\n",
    "                model_args[k]=v.format(dataset,scale,f\"x{i}\",step)\n",
    "            trainer = EnhancedNERTrainer(**model_args)\n",
    "\n",
    "            for _ in trainer(lr2=1e-2):\n",
    "                pass\n",
    "\n",
    "            torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
